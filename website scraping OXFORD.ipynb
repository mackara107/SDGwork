{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.error, urllib.parse\n",
    "import requests\n",
    "import csv\n",
    "from urllib.request import build_opener, HTTPCookieProcessor, Request\n",
    "\n",
    "#PUT JOURNAL URL HERE\n",
    "url = 'https://academic.oup.com/jcr/issue-archive'\n",
    "\n",
    "\n",
    "opener = build_opener(HTTPCookieProcessor())\n",
    "request = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "response = opener.open(request, timeout=30)\n",
    "allContent = str(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSheetHeader(name):\n",
    "    # field names  \n",
    "    fields = ['Title', 'Abstract','Journal']  \n",
    "    \n",
    "    # name of csv file  \n",
    "    filename = name + \" SCRAPED.csv\"\n",
    "    \n",
    "    # writing to csv file  \n",
    "    with open(filename, 'w') as csvfile:  \n",
    "        # creating a csv writer object  \n",
    "        csvwriter = csv.writer(csvfile)  \n",
    "        \n",
    "        # writing the fields  \n",
    "        csvwriter.writerow(fields)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSheet(rows,name):\n",
    "    # name of csv file  \n",
    "    filename = name + \" SCRAPED.csv\"\n",
    "    \n",
    "    # writing to csv file  \n",
    "    with open(filename, 'a') as csvfile:  \n",
    "        # creating a csv writer object  \n",
    "        csvwriter = csv.writer(csvfile)  \n",
    "        \n",
    "        # writing the data rows  \n",
    "        csvwriter.writerows(rows) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'DOCTYPE html>\\\\r \\\\r  SCRAPED.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-49a542c796f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mjournal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjournal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtoSheetHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjournal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#FINDING THE LINK TO THE ISSUE, allContent is all of the code for the original URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-9ba0f27fb7cd>\u001b[0m in \u001b[0;36mtoSheetHeader\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# writing to csv file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;31m# creating a csv writer object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mcsvwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'DOCTYPE html>\\\\r \\\\r  SCRAPED.csv'"
     ]
    }
   ],
   "source": [
    "allarticles = []\n",
    "\n",
    "\n",
    "#GETTING JOURNAL TITLE\n",
    "\n",
    "\n",
    "journal_index = allContent.find('<option class=\"header-search-bar-filters-item\"')\n",
    "journal_index = allContent.find('>',journal_index) + 5\n",
    "journal = \"\"\n",
    "while(allContent[journal_index]!=\"<\"):\n",
    "    journal = journal + allContent[journal_index]\n",
    "    journal_index = journal_index+1\n",
    "\n",
    "\n",
    "journal = journal.replace(\"\\\\n\",\" \")\n",
    "\n",
    "toSheetHeader(journal)\n",
    "                    \n",
    "#FINDING THE LINK TO THE ISSUE, allContent is all of the code for the original URL                    \n",
    "issue_index = 0\n",
    "\n",
    "dates = [\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"]\n",
    "\n",
    "for d in dates:\n",
    "    issue_link = \"https://academic.oup.com/jcr/issue-archive/\"+d\n",
    "    \n",
    "    #getting code for ISSUE LINK here\n",
    "    opener = build_opener(HTTPCookieProcessor())\n",
    "    request = Request(issue_link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    response = opener.open(request, timeout=30)\n",
    "    web1Content = str(response.read())\n",
    "\n",
    "    \n",
    "    #finding issue\n",
    "    curr_index = 0\n",
    "    while(web1Content.find('/jcr/issue/',curr_index,len(web1Content))!=-1):\n",
    "        curr_index = web1Content.find('/jcr/issue/',curr_index,len(web1Content))\n",
    "        \n",
    "        \n",
    "        iss_link_index = web1Content.find(\"/jcr/issue/\",curr_index,len(web1Content))\n",
    "        iss_link = \"\"\n",
    "        while(web1Content[curr_index]!='\"'):\n",
    "            iss_link = iss_link+web1Content[curr_index]\n",
    "            curr_index = curr_index+1\n",
    "           \n",
    "        \n",
    "        opener = build_opener(HTTPCookieProcessor())\n",
    "        request = Request(\"https://academic.oup.com\"+iss_link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response = opener.open(request, timeout=30)\n",
    "        webContent = str(response.read())\n",
    "    \n",
    "    \n",
    "        article_index = 0  \n",
    "        ind = 0\n",
    "        while(webContent.find('<div class=\"al-article-items\">',article_index,len(webContent))!=-1):\n",
    "            currarticle = []\n",
    "            article_index = webContent.find('<div class=\"al-article-items\">',article_index,len(webContent))\n",
    "        \n",
    "            link_index = webContent.find(\"href=\",article_index,len(webContent))\n",
    "            link = \"\"\n",
    "            \n",
    "            i = link_index+6\n",
    "            while(webContent[i]!='\"'):\n",
    "                link = link+webContent[i]\n",
    "                i = i+1\n",
    "\n",
    "            title = \"\"\n",
    "      \n",
    "            i = i+2\n",
    "            \n",
    "            while(webContent[i]!='<'):\n",
    "                title = title+webContent[i]\n",
    "                i = i+1\n",
    "\n",
    "            title = title.replace(\"\\\\n\",\"\")\n",
    "            currarticle.append(title)\n",
    "            #print(\"\\033[1m\",title,\"\\033[0;0m\")\n",
    "\n",
    "        \n",
    "      \n",
    "            #OPENING LINK TO ARTICLE\n",
    "            opener = build_opener(HTTPCookieProcessor())\n",
    "            request = Request(\"https://academic.oup.com\"+link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response = opener.open(request, timeout=30)\n",
    "            articleContent = str(response.read())\n",
    "            print(\"https://academic.oup.com\"+link)\n",
    "        \n",
    "            execute = True\n",
    "            abstract_index = articleContent.find('<section class=\"abstract\">')\n",
    "           \n",
    "            if (abstract_index == -1):\n",
    "                abstract_index = articleContent.find('article-section__content')\n",
    "            if (abstract_index == -1):\n",
    "                abstract_index = articleContent.find('<div class=\"section1')\n",
    "            if (abstract_index == -1):\n",
    "                execute = False\n",
    "            print(abstract_index)\n",
    "            \n",
    "            if(execute):    \n",
    "                abstract_index = articleContent.find(\"<p\",abstract_index,len(articleContent))\n",
    "                abstract_index = articleContent.find(\">\",abstract_index,len(articleContent))\n",
    "                i = abstract_index+1\n",
    "      \n",
    "                end_abstract_index = articleContent.find(\"</p>\",abstract_index,len(articleContent))\n",
    "\n",
    "                abstract = \"\"\n",
    "\n",
    "                while(i!=end_abstract_index):\n",
    "                    abstract = abstract+articleContent[i]\n",
    "                    i = i + 1\n",
    "    \n",
    "                abstract = abstract.replace(\"\\\\n\",\" \")\n",
    "                currarticle.append(abstract)\n",
    "\n",
    "            currarticle.append(journal)\n",
    "            allarticles.append(currarticle)\n",
    "            toSheet(allarticles,journal)\n",
    "            allarticles=[]\n",
    "            article_index = article_index+1\n",
    "            issue_index = issue_index+1\n",
    "            curr_index = curr_index +1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
