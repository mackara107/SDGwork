{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.error, urllib.parse\n",
    "import requests\n",
    "import csv\n",
    "from urllib.request import build_opener, HTTPCookieProcessor, Request\n",
    "from scholarly import scholarly\n",
    "import time\n",
    "import random\n",
    "import pdfkit\n",
    "\n",
    "nrf_issn = \"1477-8947\"\n",
    "jbe_issn = \"1573-0697\"\n",
    "hess_issn = \"1607-7938\"\n",
    "test_issn = '2325-1077'\n",
    "\n",
    "url = 'https://api.crossref.org/journals/'+test_issn+'/works?filter=from-pub-date:2015-01-01&rows=1000'\n",
    "\n",
    "print(url)\n",
    "\n",
    "opener = build_opener(HTTPCookieProcessor())\n",
    "request = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "response = opener.open(request, timeout=30)\n",
    "allContent = str(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_i = allContent.find('total-results')\n",
    "results_i = allContent.find(':',results_i) + 1\n",
    "results = \"\"\n",
    "while(allContent[results_i]!=','):\n",
    "    results = results + allContent[results_i]\n",
    "    results_i = results_i+1\n",
    "print(results,\"Total Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>toSheetHeader creates the csv file and its header<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSheetHeader(name):\n",
    "    fields = ['Title', 'Abstract','Journal']  \n",
    "    \n",
    "    # name of csv file  \n",
    "    filename = name + \" SCRAPED.csv\"\n",
    "      \n",
    "    with open(filename, 'w') as csvfile: \n",
    "        csvwriter = csv.writer(csvfile)   \n",
    "        csvwriter.writerow(fields)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>toSheet writes a new row to the csv file<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSheet(rows,name): \n",
    "    filename = name + \" SCRAPED.csv\"\n",
    "     \n",
    "    with open(filename, 'a') as csvfile: \n",
    "        csvwriter = csv.writer(csvfile)  \n",
    "        csvwriter.writerows(rows) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Offset function to get the next page of 1000 results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset(i, url):\n",
    "    url = url + \"&offset=\"+str(i)\n",
    "    \n",
    "    print(url)\n",
    "    \n",
    "    loop=True\n",
    "    while(loop):\n",
    "        try:\n",
    "            opener = build_opener(HTTPCookieProcessor())\n",
    "            request = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response = opener.open(request, timeout=30)\n",
    "            allContent = str(response.read())\n",
    "            loop=False\n",
    "        except:\n",
    "            print(\"error loading next page\")\n",
    "            \n",
    "    return(allContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>findAbstract method returns the abstract index or -1 if not found.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAbstract(articleContent):\n",
    "    abstract_index = articleContent.find('abstractInFull')\n",
    "    if (abstract_index == -1):\n",
    "        abstract_index = articleContent.find('<div class=\"abstract\">')\n",
    "    if (abstract_index == -1):\n",
    "        abstract_index = articleContent.find('Abs1-content')\n",
    "    if (abstract_index == -1):\n",
    "        abstract_index = articleContent.find('\"abstract-content')\n",
    "    if (abstract_index == -1):\n",
    "        abstract_index = articleContent.find('article-section__content')\n",
    "    if (abstract_index == -1):\n",
    "        abstract_index = articleContent.find('abstr1')\n",
    "    if (abstract_index == -1):\n",
    "        abstract_index = articleContent.find('<div class=\"section1')\n",
    "    if (abstract_index == -1):\n",
    "        abstract_index = articleContent.find('section id=\"abstract\"')\n",
    "    if (abstract_index == -1):\n",
    "        abstract_index = articleContent.find('<p id=\"abspara0010\">')\n",
    "    if (abstract_index == -1):\n",
    "        abstract_index = articleContent.find('<div id=\"abssec0010\">')\n",
    "    if (abstract_index == -1):\n",
    "        abstract_index = articleContent.find('<section class=\"abstract\">')\n",
    "    if (abstract_index == -1):\n",
    "        abstract_index = articleContent.find('<div id=\"as010\">')\n",
    "    if (abstract_index == -1):\n",
    "        abstract_index = articleContent.find('<div class=\"product-specs\">')\n",
    "    if (abstract_index == -1):\n",
    "        abstract_index = articleContent.find('<div class=\"mod abstract\">')\n",
    "        \n",
    "               \n",
    "    \n",
    "    return abstract_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Scholar uses Google Scholar to find the abstract</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scholar(title):\n",
    "    print(\"using google scholar\")\n",
    "    \n",
    "    time.sleep(30) # Sleep for 30 seconds\n",
    "    \n",
    "    search_query = scholarly.search_pubs(title)\n",
    "    scholar = str(next(search_query))\n",
    "    index = scholar.find(\"'abstract'\") + 13\n",
    "\n",
    "    abstract = \"\"\n",
    "    while(scholar[index]!=\"'\"):\n",
    "        abstract = abstract + scholar[index]\n",
    "        index = index + 1\n",
    "    return(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Getting Journal Title</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_index = allContent.find('\"container-title\"')\n",
    "journal_index = allContent.find('[',journal_index) + 2\n",
    "journal = \"\"\n",
    "while(allContent[journal_index]!='\"'):\n",
    "    journal = journal + allContent[journal_index]\n",
    "    journal_index = journal_index+1\n",
    "print(journal)\n",
    "toSheetHeader(journal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Getting Journal Article Titles and Abstracts</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "loop = True\n",
    "\n",
    "while(loop):\n",
    "    index = 0\n",
    "    if(i%1000==0 and allContent.find('indexed',index,len(allContent))==-1):\n",
    "        loop = False\n",
    "    while(allContent.find('indexed',index,len(allContent))!=-1):\n",
    "        index = allContent.find('indexed',index,len(allContent))\n",
    "        title_index = allContent.find('\"title\":',index,len(allContent))\n",
    "        title_index = allContent.find('[',title_index) + 2\n",
    "        title = \"\"\n",
    "        while(allContent[title_index]!='\"'):\n",
    "            title = title + allContent[title_index]\n",
    "            title_index = title_index+1\n",
    "        \n",
    "        print(str(i+1)+\". \"+title)\n",
    "        \n",
    "        nexttitle = allContent.find('indexed',title_index,len(allContent))\n",
    "        if(nexttitle==-1):\n",
    "            nexttitle=len(allContent)\n",
    "        \n",
    "        link = \"\"\n",
    "        abstract = \"\"\n",
    "        \n",
    "        if (allContent.find('\"abstract\"',index,nexttitle)!=-1):\n",
    "            #if journal supplied abstract to CrossRef\n",
    "            index = allContent.find('\"abstract\"',index,nexttitle)+12\n",
    "            endindex = allContent.find('\",',index,nexttitle)\n",
    "            while(index<endindex):\n",
    "                abstract = abstract + allContent[index]\n",
    "                index = index+1\n",
    "        else:\n",
    "            #if journal didn't supply, using URL\n",
    "            first_link = allContent.find('\"link\"',index,nexttitle)\n",
    "            if(first_link!=-1):\n",
    "                index = allContent.find('\"URL\"',first_link,nexttitle)\n",
    "            else:\n",
    "                index = allContent.find('\"URL\"',index,nexttitle)\n",
    "            index = allContent.find('http',index)\n",
    "            while(allContent[index]!='\"'):\n",
    "                link = link + allContent[index]\n",
    "                index = index+1\n",
    "            link = link.replace(\"\\\\\\\\\",\"\")\n",
    "            \n",
    "            if \"pdf\" in link:\n",
    "                link = \"\"\n",
    "                index = allContent.find('http',index)\n",
    "                while(allContent[index]!='\"'):\n",
    "                    link = link + allContent[index]\n",
    "                    index = index+1\n",
    "                link = link.replace(\"\\\\\\\\\",\"\")\n",
    "                \n",
    "            if \"/xml\" in link:\n",
    "                link = \"\"\n",
    "                index = allContent.find('http',index)\n",
    "                while(allContent[index]!='\"'):\n",
    "                    link = link + allContent[index]\n",
    "                    index = index+1\n",
    "                link = link.replace(\"\\\\\\\\\",\"\")\n",
    "            \n",
    "            tryloop = True\n",
    "            articleContent = \"\"\n",
    "            while tryloop:\n",
    "                try:\n",
    "                    randomtime = random.randint(0,100)\n",
    "                    print(\"Delaying\",randomtime,\"Seconds...\")\n",
    "                    time.sleep(randomtime)\n",
    "\n",
    "                    tryloop = False\n",
    "                    opener = build_opener(HTTPCookieProcessor())\n",
    "                    request = Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                    response = opener.open(request, timeout=30)\n",
    "                    articleContent = str(response.read())\n",
    "                except:\n",
    "                    tryloop = True\n",
    "                    \n",
    "                    test = allContent.find('\"URL\"',index,nexttitle)\n",
    "                    if (test == -1):\n",
    "                        tryloop = False\n",
    "                        print(\"Can't open link.\")\n",
    "                    else:\n",
    "                        link = \"\"\n",
    "                        index = test\n",
    "                        index = allContent.find('http',index)\n",
    "                        while(allContent[index]!='\"'):\n",
    "                            link = link + allContent[index]\n",
    "                            index = index+1\n",
    "                        link = link.replace(\"\\\\\\\\\",\"\")\n",
    "                    \n",
    "            \n",
    "            print(link)\n",
    "            \n",
    "            abstract_index = findAbstract(articleContent)\n",
    "            \n",
    "            #if(i==1):\n",
    "                #print(articleContent)\n",
    "            \n",
    "            if(abstract_index!=-1):    \n",
    "                abstract_index = articleContent.find(\">\",abstract_index,len(articleContent))+2\n",
    "                x = abstract_index+1\n",
    "                end_abstract_index = articleContent.find(\"</p>\",abstract_index,len(articleContent))\n",
    "\n",
    "                abstract = \"\"\n",
    "\n",
    "                while(x!=end_abstract_index):\n",
    "                    abstract = abstract+articleContent[x]\n",
    "                    x = x + 1\n",
    "            #else:\n",
    "                #pdfkit.from_url(link,title+\" PDF\")\n",
    "                #print(\"saved as pdf\")\n",
    "                #except:\n",
    "                    #print(\"pdf failed\")\n",
    "            #else:\n",
    "                #try:\n",
    "                    #abstract = scholar(title)\n",
    "                #except:\n",
    "                    #print(\"scholar failed\")\n",
    "\n",
    "        abstract = abstract.replace(\"\\\\n\",\" \")\n",
    "                      \n",
    "            \n",
    "        index=index+1\n",
    "        i = i + 1\n",
    "        \n",
    "        \n",
    "        print(abstract)\n",
    "        print(str(len(abstract))+\"\\n\\n\")\n",
    "        \n",
    "        toSheet([[title,abstract,journal]],journal)\n",
    "        \n",
    "        \n",
    "        \n",
    "    #GETTING NEXT PAGE OF CONTENT IF AVAILABLE  \n",
    "    if(i%1000==0):\n",
    "        print(\"\\n\")\n",
    "        allContent = offset(i, url)\n",
    "        #print(offset)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        loop=False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
